{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b635b6-483c-40d6-add0-f0699a27f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, Distributions, LinearAlgebra, Plots\n",
    "\n",
    "rng = Random.seed!(1234)\n",
    "\n",
    "\n",
    "ϵ = 1.0e-6  # to help with numerical stability in matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646b07d-00e9-45e3-9604-0f9b29e3efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "function SE_kernel(x, x′; λ=1, ℓ=1)\n",
    "    return (λ^2) * exp( -(x-x′)^2 / (2*(ℓ^2)) )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe4968-afeb-4bc7-baee-de5bad114ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We separate the calculation of the covariance matrix into two functions to take\n",
    "# advantage of the symmetry found in covariance matrices to minimize some computation\n",
    "\n",
    "function self_cov_mat(kernel, X)\n",
    "\n",
    "    len = length(X)\n",
    "    K = zeros(len, len)\n",
    "    for i in 1:len\n",
    "        for j in i:len\n",
    "            K[i,j] = kernel(X[i], X[j])\n",
    "            K[j,i] = K[i,j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return K + ϵ * I # for numerical stability\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "function cross_cov_mat(kernel, X, X′)\n",
    "    \n",
    "    size= length(X)\n",
    "    size′= length(X′)\n",
    "    K =zeros(size, size′)\n",
    "    \n",
    "    for i in 1:size\n",
    "        for j in 1:size′\n",
    "            K[i,j] = kernel(X[i], X′[j])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return K\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2aa5e-5c41-4a9a-856b-6133487b6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prior\n",
    "# Here we build a method to draw samples from the prior distribution of our GP.\n",
    "\n",
    "function sample_prior(xs, num_samples, kernel)\n",
    "\n",
    "    μ = zeros(length(xs))\n",
    "    Σ = self_cov_mat(kernel, xs)\n",
    "    \n",
    "    ys = rand(MvNormal(μ, Σ), num_samples)\n",
    "\n",
    "    plot(xs, ys)\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d6302-7e5f-466a-8db4-c959548dd323",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = range(-5, 35; length=1000)\n",
    "sample_prior(xs, 3, SE_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47891cc2-4d51-446e-ab9d-f45d2f1792ee",
   "metadata": {},
   "source": [
    "Intuitively, we can think of each of these samples as a function, and so our GP is just a multivariate Gaussian over the function space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f89317-a186-4d3a-8c1c-a54527132f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictive distribution for y*\n",
    "\n",
    "function predictive_gp(X, y, Xs , kernel)\n",
    "\n",
    "    KXX = self_cov_mat(kernel, X) \n",
    "    KXsX = cross_cov_mat(kernel, Xs, X)\n",
    "    KXsXs = self_cov_mat(kernel, Xs)\n",
    "      \n",
    "    μ_pred = KXsX * inv(KXX) * y\n",
    "    K_pred = KXsXs - KXsX * inv(KXX) * transpose(KXsX)\n",
    "    \n",
    "    return (μ_pred, Symmetric(K_pred))     # Symmetric() required due to some numeric instability\n",
    "   \n",
    "end\n",
    "\n",
    "\n",
    "function sample_predictive(x_train, y_train, x_pred, num_samples, kernel)\n",
    "\n",
    "    μ, K = predictive_gp(x_train, y_train, x_pred, kernel)\n",
    "    ys = rand(MvNormal(μ, K), num_samples)\n",
    "    σs = sqrt.(diag(K))\n",
    "\n",
    "    return ys, σs\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9611e-a0e6-4d1a-9723-d37f2c788dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Data\n",
    "\n",
    "# We will start off by making our toy dataset, by adding \n",
    "# some noise to a nonlinear function.\n",
    "\n",
    "num_sample = 40\n",
    "num_train = 30\n",
    "\n",
    "x = range(0,30;length=num_sample);\n",
    "ϵ_data = rand(Normal(0,0.5), num_sample);\n",
    "y = sqrt.(x).*sin.(x) .+ ϵ_data;\n",
    "plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32862f42-cbf6-41cb-98c7-ae1cdd0a1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have split our data to isolate some training data, as well as some unseen data. We are\n",
    "# calling this unseen data \"test\" data.\n",
    "\n",
    "indices = randcycle(rng, num_sample)\n",
    "x_train = zeros(num_train)\n",
    "y_train = zeros(num_train)\n",
    "x_test = zeros(num_sample-num_train)\n",
    "y_test = zeros(num_sample-num_train)\n",
    "\n",
    "for (i, val) in enumerate(indices)\n",
    "    if i<=num_train\n",
    "        x_train[i] = x[val]\n",
    "        y_train[i] = y[val]\n",
    "    else\n",
    "        x_test[i-num_train] = x[val]\n",
    "        y_test[i-num_train] = y[val]\n",
    "    end\n",
    "end\n",
    "scatter!(x_train ,y_train,label=\"training\")\n",
    "scatter!(x_test ,y_test,label=\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c23de-c74c-4398-a268-fe10bf5ba2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, run our model and make 'predictions'\n",
    "x_pred = range(-5, 35; length=1000)\n",
    "y_pred, σ_pred = sample_predictive(x_train, y_train, x_pred, 3, SE_kernel)\n",
    "\n",
    "\n",
    "plot(x_pred, y_pred, ribbon=σ_pred, label=\"prediction\")\n",
    "scatter!(x_train, y_train, label=\"training\")\n",
    "scatter!(x_test, y_test, label=\"testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d0364-82be-420b-9cd5-a7454a565a76",
   "metadata": {},
   "source": [
    "## Length scale parameter $l$\n",
    "\n",
    "Let's next try a different (and much more quickly varying) target function. We edit the (hardcoded) hyperparameters of our kernel function to better match our assumptions/knowledge about the function we are trying to model. Ideally, these hyperparameters should normally be learned and tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427d1ee-34e5-4751-9cc0-d864a2d6f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!! Play around with the value of ℓ !!!!\n",
    "function SE_kernel_2(x, x′; λ=1, ℓ=0.025)\n",
    "    \n",
    "    return (λ^2) * exp( -(x-x′)^2 / (2*(ℓ^2)) )\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77689964-6333-4a93-adad-01d2eb2131c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 50\n",
    "num_train = 40\n",
    "\n",
    "x = range(0,1;length=num_sample)\n",
    "ϵ_data = rand(Normal(0,0.5), num_sample);\n",
    "y = ℯ.^(x) - 3*tan.(x) .+ ϵ_data;\n",
    "\n",
    "plot(x,y)\n",
    "\n",
    "\n",
    "indices = randcycle(rng, num_sample)\n",
    "x_train = zeros(num_train)\n",
    "y_train = zeros(num_train)\n",
    "x_test = zeros(num_sample-num_train)\n",
    "y_test = zeros(num_sample-num_train)\n",
    "\n",
    "for (i, val) in enumerate(indices)\n",
    "    if i<=num_train\n",
    "        x_train[i] = x[val]\n",
    "        y_train[i] = y[val]\n",
    "    else\n",
    "        x_test[i-num_train] = x[val]\n",
    "        y_test[i-num_train] = y[val]\n",
    "    end\n",
    "end\n",
    "\n",
    "scatter!(x_train ,y_train,label=\"training\")\n",
    "scatter!(x_test ,y_test,label=\"testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658d07e-d2b6-4141-99c9-7f97922b0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = range(0,1;length=1000)\n",
    "sample_prior(xs, 3, SE_kernel_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a267f62-0e87-4457-a7d5-8e44aae2cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = range(0, 1; length=1000)\n",
    "y_pred, σ_pred = sample_predictive(x_train, y_train, x_pred, 3, SE_kernel_2)\n",
    "\n",
    "plot(x_pred, y_pred, ribbon=σ_pred, label=\"prediction\")\n",
    "scatter!(x_train ,y_train,label=\"training\")\n",
    "scatter!(x_test ,y_test,label=\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13720fc4-d15c-4ed1-a03d-d9fa4c8e4005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
